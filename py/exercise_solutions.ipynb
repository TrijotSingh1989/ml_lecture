{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises with Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Basics and Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise on linear regression\n",
    "\n",
    "As alternative to the multiple linear regression on diamond prices with logarithmic price and logarithmic carat, consider the same model without logarithms. Interpret the output of the model. Does it make sense from a practical perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1156.648\n",
      "R-squared: 91.59%\n",
      "Intercept -7362.802156301801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>carat</th>\n",
       "      <td>8886.128883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_E</th>\n",
       "      <td>-211.682481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_F</th>\n",
       "      <td>-303.310033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_G</th>\n",
       "      <td>-506.199536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_H</th>\n",
       "      <td>-978.697665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_I</th>\n",
       "      <td>-1440.301902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_J</th>\n",
       "      <td>-2325.222360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_Good</th>\n",
       "      <td>655.767448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_Very Good</th>\n",
       "      <td>848.716878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_Premium</th>\n",
       "      <td>869.395903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_Ideal</th>\n",
       "      <td>998.254438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_SI2</th>\n",
       "      <td>2625.949987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_SI1</th>\n",
       "      <td>3573.687987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_VS2</th>\n",
       "      <td>4217.829102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_VS1</th>\n",
       "      <td>4534.878970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <td>4967.199410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <td>5072.027645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity_IF</th>\n",
       "      <td>5419.646845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Estimates\n",
       "carat          8886.128883\n",
       "color_E        -211.682481\n",
       "color_F        -303.310033\n",
       "color_G        -506.199536\n",
       "color_H        -978.697665\n",
       "color_I       -1440.301902\n",
       "color_J       -2325.222360\n",
       "cut_Good        655.767448\n",
       "cut_Very Good   848.716878\n",
       "cut_Premium     869.395903\n",
       "cut_Ideal       998.254438\n",
       "clarity_SI2    2625.949987\n",
       "clarity_SI1    3573.687987\n",
       "clarity_VS2    4217.829102\n",
       "clarity_VS1    4534.878970\n",
       "clarity_VVS2   4967.199410\n",
       "clarity_VVS1   5072.027645\n",
       "clarity_IF     5419.646845"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Via scikit-learn\n",
    "from plotnine.data import diamonds\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "y = diamonds[\"price\"]\n",
    "cat_vars = [\"color\", \"cut\", \"clarity\"]\n",
    "lvl = [diamonds[x].cat.categories.to_numpy() for x in cat_vars]\n",
    "\n",
    "model = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"linear\", \"passthrough\", [\"carat\"]),\n",
    "            (\"dummies\", OneHotEncoder(categories=lvl, drop=\"first\"), cat_vars)\n",
    "        ],\n",
    "        verbose_feature_names_out=False\n",
    "    ),\n",
    "    LinearRegression()\n",
    ")\n",
    "model.fit(diamonds, y)\n",
    "\n",
    "print(f\"RMSE: {mse(y, model.predict(diamonds), squared=False):.3f}\")\n",
    "print(f\"R-squared: {model.score(diamonds, y):.2%}\")\n",
    "print(\"Intercept\", model[-1].intercept_)\n",
    "\n",
    "feature_names = model[:-1].get_feature_names_out()\n",
    "results = pd.DataFrame(\n",
    "    model[-1].coef_,\n",
    "    columns=[\"Estimates\"],\n",
    "    index=feature_names\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via statsmodels\n",
    "from plotnine.data import diamonds\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model2 = ols(\"price ~ carat + color + cut + clarity\", data=diamonds)\n",
    "results2 = model2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- **Model quality:** About 92% of price variations are explained by covariates.\n",
    " Typical prediction error is 1157 USD.\n",
    "- **Effects:** All effects point into the intuitively right direction\n",
    " (larger stones are more expensive, worse color are less expensive etc.)\n",
    "- **Practical perspective:** Additivity in color, cut and clarity are not \n",
    " making sense. Their effects should get larger with larger diamond size. \n",
    " This can be solved by adding interaction terms with carat or, much easier,\n",
    " to switch to a logarithmic response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise on GLMs\n",
    "\n",
    "Fit a Gamma regression with log-link to explain diamond prices by `log(carat)`, `color`, `cut`, and `clarity`. Compare the coefficients with those from the corresponding linear regression with `log(price)` as response. Use dummy coding for the three categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine.data import diamonds\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.glm(\n",
    "    \"price ~ np.log(carat) + color + cut + clarity\", data=diamonds, \n",
    "    family=sm.families.Gamma(sm.families.links.log())\n",
    ")\n",
    "result = model.fit()\n",
    "result.summary()\n",
    "\n",
    "bias = diamonds[\"price\"].mean() / result.predict(diamonds).mean() - 1\n",
    "print(f\"Relative bias on USD scale: {bias:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The coefficients are very similar to the linear regression with\n",
    " log(price) as response. This makes sense, in the end we interpret the \n",
    " coefficients in the same way! The bias is only 0.3%, i.e. much smaller\n",
    " than the 3% of the OLS with log(price) as response. Still, because\n",
    " log is not the natural link of the Gamma regression, it is not exactly 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Model Selection and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Apply *stratified* splitting (on the response `price`) throughout the \n",
    "last example. Do the results change?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(FNN)\n",
    "library(splitTools)\n",
    "library(MetricsWeighted)\n",
    "\n",
    "# Covariates\n",
    "x <- c(\"carat\", \"color\", \"cut\", \"clarity\")\n",
    "\n",
    "dia <- diamonds[, c(\"price\", x)]\n",
    "\n",
    "# Split diamonds into 90% for training and 10% for testing\n",
    "ix <- partition(dia$price, p = c(train = 0.9, test = 0.1), \n",
    "                seed = 9838, type = \"stratified\")\n",
    "\n",
    "train <- dia[ix$train, ]\n",
    "test <- dia[ix$test, ]\n",
    "\n",
    "y_train <- train$price\n",
    "y_test <- test$price\n",
    "\n",
    "# Standardize training data\n",
    "X_train <- scale(data.matrix(train[, x]))\n",
    "\n",
    "# Apply training scale to test data\n",
    "X_test <- scale(data.matrix(test[, x]),\n",
    "                center = attr(X_train, \"scaled:center\"),\n",
    "                scale = attr(X_train, \"scaled:scale\"))\n",
    "\n",
    "# Split training data into folds\n",
    "nfolds <- 5\n",
    "folds <- create_folds(y_train, k = nfolds, seed = 9838, type = \"stratified\")\n",
    "\n",
    "# Cross-validation performance of k-nearest-neighbour for k = 1-20\n",
    "paramGrid <- data.frame(RMSE = NA, k = 1:20)\n",
    "\n",
    "for (i in 1:nrow(paramGrid)) {\n",
    "  k <- paramGrid[i, \"k\"]\n",
    "  scores <- c()\n",
    "  \n",
    "  for (fold in folds) {\n",
    "    pred <- knn.reg(X_train[fold, ], test = X_train[-fold, ], \n",
    "                    k = k, y = y_train[fold])$pred\n",
    "    scores[length(scores) + 1] <- rmse(y_train[-fold], pred)\n",
    "  }\n",
    "  paramGrid[i, \"RMSE\"] <- mean(scores)\n",
    "}\n",
    "\n",
    "# Best k along with its CV performance\n",
    "paramGrid[order(paramGrid$RMSE)[1], ]\n",
    "\n",
    "# Cross-validation performance of linear regression\n",
    "rmse_reg <- c()\n",
    "\n",
    "for (fold in folds) {\n",
    "  fit <- lm(price ~ log(carat) + color + cut + carat, data = train[fold, ])\n",
    "  pred <- predict(fit, newdata = train[-fold, ])\n",
    "  rmse_reg[length(rmse_reg) + 1] <- rmse(y_train[-fold], pred)\n",
    "}\n",
    "(rmse_reg <- mean(rmse_reg))\n",
    "\n",
    "# The overall best model is 6-nearest-neighbour\n",
    "pred <- knn.reg(X_train, test = X_test, k = 6, y = y_train)$pred\n",
    "\n",
    "# Test performance for the best model\n",
    "rmse(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The CV results are slightly better and the test performance is clearly worse. Both might be by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Regarding the problem that some diamonds seem to appear multiple times in \n",
    "the data: As an alternative to *grouped* splitting, repeat the last example \n",
    "also on data deduplicated by `price` and all covariates. Do the results change? \n",
    "Do you think these results are overly pessimistic?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(FNN)\n",
    "library(splitTools)\n",
    "library(MetricsWeighted)\n",
    "\n",
    "# Covariates\n",
    "x <- c(\"carat\", \"color\", \"cut\", \"clarity\")\n",
    "\n",
    "dia <- diamonds[, c(\"price\", x)]\n",
    "dia <- unique(dia)\n",
    "\n",
    "# Split diamonds into 90% for training and 10% for testing\n",
    "ix <- partition(dia$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = \"basic\")\n",
    "\n",
    "train <- dia[ix$train, ]\n",
    "test <- dia[ix$test, ]\n",
    "\n",
    "y_train <- train$price\n",
    "y_test <- test$price\n",
    "\n",
    "# Standardize training data\n",
    "X_train <- scale(data.matrix(train[, x]))\n",
    "\n",
    "# Apply training scale to test data\n",
    "X_test <- scale(data.matrix(test[, x]),\n",
    "                center = attr(X_train, \"scaled:center\"),\n",
    "                scale = attr(X_train, \"scaled:scale\"))\n",
    "\n",
    "# Split training data into folds\n",
    "nfolds <- 5\n",
    "folds <- create_folds(y_train, k = nfolds, seed = 9838, type = \"basic\")\n",
    "\n",
    "# Cross-validation performance of k-nearest-neighbour for k = 1-20\n",
    "paramGrid <- data.frame(RMSE = NA, k = 1:20)\n",
    "\n",
    "for (i in 1:nrow(paramGrid)) {\n",
    "  k <- paramGrid[i, \"k\"]\n",
    "  scores <- c()\n",
    "  \n",
    "  for (fold in folds) {\n",
    "    pred <- knn.reg(X_train[fold, ], test = X_train[-fold, ], \n",
    "                    k = k, y = y_train[fold])$pred\n",
    "    scores[length(scores) + 1] <- rmse(y_train[-fold], pred)\n",
    "  }\n",
    "  paramGrid[i, \"RMSE\"] <- mean(scores)\n",
    "}\n",
    "\n",
    "# Best k along with its CV performance\n",
    "paramGrid[order(paramGrid$RMSE)[1], ]\n",
    "\n",
    "# Cross-validation performance of linear regression\n",
    "rmse_reg <- c()\n",
    "\n",
    "for (fold in folds) {\n",
    "  fit <- lm(price ~ log(carat) + color + cut + carat, data = train[fold, ])\n",
    "  pred <- predict(fit, newdata = train[-fold, ])\n",
    "  rmse_reg[length(rmse_reg) + 1] <- rmse(y_train[-fold], pred)\n",
    "}\n",
    "(rmse_reg <- mean(rmse_reg))\n",
    "\n",
    "# The overall best model is 5-nearest-neighbour\n",
    "pred <- knn.reg(X_train, test = X_test, k = 5, y = y_train)$pred\n",
    "\n",
    "# Test performance for the best model\n",
    "rmse(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:** The test performance of the best model (5-nn) seems clearly worse than the one\n",
    "without deduplication (~700 USD RMSE vs ~600). CV performance well corresponds to test performance.\n",
    "Overall, this is probably the more realistic performance than the one obtained from the original data set.\n",
    "Still, as certain rows could be identical by chance, our deduplication approach might be slightly too conservative.\n",
    "The true performance will probably be somewhere between the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Use cross-validation to select the best polynomial degree to represent \n",
    "`log(carat)` in the Gamma GLM with log-link (with additional covariates \n",
    "`color`, `cut`, and `clarity`). Evaluate the result on an independent \n",
    "test data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(splitTools)\n",
    "library(MetricsWeighted)\n",
    "\n",
    "dia <- diamonds %>% \n",
    "  mutate_at(c(\"color\", \"cut\", \"clarity\"), function(z) factor(z, ordered = FALSE))\n",
    "\n",
    "# Split diamonds into 90% for training and 10% for testing\n",
    "ix <- partition(dia$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = \"basic\")\n",
    "train <- dia[ix$train, ]\n",
    "test <- dia[ix$test, ]\n",
    "\n",
    "# manual GridSearchCV\n",
    "nfolds <- 5\n",
    "folds <- create_folds(train$price, k = nfolds, seed = 9838, type = \"basic\")\n",
    "paramGrid <- data.frame(Deviance = NA, k = 1:12)\n",
    "\n",
    "for (i in 1:nrow(paramGrid)) {\n",
    "  k <- paramGrid[i, \"k\"]\n",
    "  scores <- c()\n",
    "  \n",
    "  for (fold in folds) {\n",
    "    fit <- glm(price ~ poly(log(carat), degree = k) + color + cut + clarity, \n",
    "               data = train[fold, ], family = Gamma(link = \"log\"))\n",
    "    \n",
    "    pred <- predict(fit, train[-fold, ], type = \"response\")\n",
    "    scores[length(scores) + 1] <- deviance_gamma(train$price[-fold], pred)\n",
    "  }\n",
    "  paramGrid[i, \"Deviance\"] <- mean(scores)\n",
    "}\n",
    "\n",
    "paramGrid\n",
    "\n",
    "# Fit model on full training data\n",
    "fit <- glm(price ~ poly(log(carat), degree = 8) + color + cut + clarity, \n",
    "           data = train, family = Gamma(link = \"log\"))\n",
    "\n",
    "# Evaluate on test\n",
    "pred <- predict(fit, test, type = \"response\")\n",
    "deviance_gamma(test$price, pred) # 0.01710076\n",
    "r_squared_gamma(test$price, pred) # 0.982464 relative deviance gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:** The optimal degree seems to be 8 with a CV deviance of 0.01575.\n",
    "There seems to be some amount of CV overfit as the deviance evaluated on \n",
    "the test data is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Optional: Compare the linear regression for `price` (using `log(carat)`, \n",
    "`color`, `cut`, and `clarity` as covariates) with a corresponding Gamma GLM \n",
    "with log-link by simple validation. Use once (R)MSE for comparison and once \n",
    "Gamma deviance. \n",
    "\n",
    "What do you observe? -> solution not shown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises on Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In above example, replace carat by its logarithm. Do the results change \n",
    "compared to the example without logs?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(splitTools)\n",
    "library(ranger)\n",
    "library(MetricsWeighted)\n",
    "\n",
    "dia <- diamonds %>% \n",
    "  mutate(log_carat = log(carat))\n",
    "\n",
    "# Train/test split\n",
    "ix <- partition(dia$price, p = c(train = 0.8, test = 0.2), seed = 9838)\n",
    "\n",
    "fit <- ranger(price ~ log_carat + color + cut + clarity, \n",
    "              num.trees = 500,\n",
    "              data = dia[ix$train, ], \n",
    "              importance = \"impurity\",\n",
    "              seed = 83)\n",
    "fit\n",
    "\n",
    "# Performance on test data\n",
    "pred <- predict(fit, dia[ix$test, ])$predictions\n",
    "rmse(dia$price[ix$test], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The results are essentially identical because log is a monotonic trafo.\n",
    "Differences might come from implementation tricks of ranger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fit a random forest on the claims data, predicting the binary variable `clm` \n",
    "by the covariates `veh_value`, `veh_body`, `veh_age`, `gender`, `area`, \n",
    "and `agecat`. Choose a suitable tree-depth by maximizing OOB error on the \n",
    "training data. Make sure to fit a *probability random forest*, i.e. \n",
    "predicting probabilities, not classes. Additionally, make sure to work with \n",
    "a relevant loss function (information/cross-entropy or Gini gain). \n",
    "Use a simple train/test split. Interpret the results by split gain importance \n",
    "and partial dependence plots."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(splitTools)\n",
    "library(ranger)\n",
    "library(MetricsWeighted)\n",
    "library(insuranceData)\n",
    "data(dataCar)\n",
    "\n",
    "# Train/test split (stratified on response)\n",
    "ix <- partition(dataCar$clm, p = c(train = 0.8, test = 0.2), seed = 9838)\n",
    "\n",
    "# Instead of systematic grid search, manually select good tree depth by OOB\n",
    "fit <- ranger(clm ~ veh_value + veh_body + veh_age + gender + area + agecat,\n",
    "              data = dataCar[ix$train, ], probability = TRUE, max.depth = 5,\n",
    "              importance = \"impurity\")\n",
    "fit # OOB prediction using Brier score (= MSE) 0.06340884 \n",
    "\n",
    "# Note: Brier score is the same as the MSE, applied to binary data. It is\n",
    "# not a bad evaluation criterion in such situation, \n",
    "# so we will use this within this example.\n",
    "pred <- predict(fit, dataCar[ix$test, ])$predictions[, 2]\n",
    "mse(dataCar[ix$test, \"clm\"], pred)  # 0.06337011\n",
    "r_squared(dataCar[ix$test, \"clm\"], pred) # 0.002069925\n",
    "\n",
    "# Alternative to Brier score: relative log-loss resp. deviance improvement\n",
    "r_squared_bernoulli(dataCar[ix$test, \"clm\"], pred) # 0.004246408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Test performance with small tree depth 5 seem to be best\n",
    "according to OOB results. When studying relative performance metrics\n",
    "like the relative deviance gain, we can see that performance of the \n",
    "model is very low. TPL claims seem to be mostly determined by bad luck,\n",
    "which makes sense."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Variable importance regarding Gini improvement\n",
    "imp <- sort(importance(fit))\n",
    "imp <- imp / sum(imp)\n",
    "barplot(imp, horiz = TRUE, col = \"orange\", cex.names = 0.8)\n",
    "\n",
    "# Partial dependence plot for the strongest predictor \"veh_value\"\n",
    "fl <- flashlight(model = fit, \n",
    "                 y = \"clm\", \n",
    "                 data = dataCar[ix$train, ], \n",
    "                 label = \"rf\", \n",
    "                 predict_function = function(m, X) predict(m, X)$predictions[, 2])\n",
    "plot(light_profile(fl, v = \"veh_value\", breaks = seq(0, 5, by = 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises on Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Study the documentation of XGBoost to figure out how to make the model \n",
    "monotonically increasing in carat. \n",
    "Test your insights without rerunning the grid search in our last example,\n",
    "i.e. just be refitting the final model. \n",
    "How does the partial dependence plot for `carat` look now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(tidyverse)\n",
    "library(xgboost)\n",
    "library(splitTools)\n",
    "\n",
    "# As XGBoost does not understand strings, we encode them by integers\n",
    "prep_xgb <- function(X, x = c(\"carat\", \"color\", \"cut\", \"clarity\")) {\n",
    "  to_int <- c(\"color\", \"cut\", \"clarity\")\n",
    "  X[, to_int] <- lapply(X[, to_int], as.integer)\n",
    "  data.matrix(X[, x])\n",
    "}\n",
    "\n",
    "# Split into train and test\n",
    "ix <- partition(diamonds$price, p = c(train = 0.8, test = 0.2), seed = 9838)\n",
    "\n",
    "y_train <- diamonds$price[ix$train]\n",
    "X_train <- prep_xgb(diamonds[ix$train, ])\n",
    "\n",
    "# XGBoost data handler\n",
    "dtrain <- xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "# Load grid and select best iteration\n",
    "grid <- readRDS(\"r/gridsearch/diamonds_xgb.rds\")\n",
    "grid <- grid[order(grid$score), ]\n",
    "\n",
    "# Monotone constraints for carat\n",
    "params <- as.list(grid[1, -(1:2)])\n",
    "params$monotone_constraints <- c(1, 0, 0, 0)\n",
    "\n",
    "# Fit final, tuned model with monotone constraints for carat\n",
    "fit <- xgb.train(\n",
    "  params = params, \n",
    "  data = dtrain, \n",
    "  nrounds = grid[1, \"iteration\"]\n",
    ")\n",
    "\n",
    "# Partial dependence plot for carat\n",
    "library(flashlight)\n",
    "fl <- flashlight(model = fit, \n",
    "                 y = \"price\", \n",
    "                 data = diamonds[ix$train, ], \n",
    "                 label = \"xgb\", \n",
    "                 predict_function = function(m, X) predict(m, prep_xgb(X)))\n",
    "\n",
    "plot(light_profile(fl, v = \"carat\", n_bins = 40)) +\n",
    "  labs(title = \"Partial dependence plot for carat\", y = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The argument is called \"monotone_constraints\". For each covariate,\n",
    "a value 0 means no constraint, a value -1 means a negative constraints,\n",
    "and a value 1 means positive constraint. Applying the constraint now leads\n",
    "to a monotonically increasing partial dependence plot. This is extremely\n",
    "useful in practice. Besides monotonic constraints, also interaction \n",
    "constraints are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Develop a strong XGBoost model for the claims data set with binary response \n",
    "`clm` and covariates `veh_value`, `veh_body`, `veh_age`, `gender`, `area`, \n",
    "and `agecat`. Use a clean cross-validation/test approach. \n",
    "Use log loss both as objective and evaluation metric. Interpret its results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We just adapt the template from the script\n",
    "library(tidyverse)\n",
    "library(xgboost)\n",
    "library(splitTools)\n",
    "library(insuranceData)\n",
    "data(dataCar)\n",
    "\n",
    "# As XGBoost does not understand strings, we encode them by integers\n",
    "prep_xgb <- function(X, x = c(\"veh_value\", \"veh_body\", \"veh_age\", \n",
    "                              \"gender\", \"area\", \"agecat\")) {\n",
    "  to_int <- c(\"veh_body\", \"gender\", \"area\")\n",
    "  X[, to_int] <- lapply(X[, to_int], as.integer)\n",
    "  data.matrix(X[, x])\n",
    "}\n",
    "\n",
    "# Split into train and test\n",
    "ix <- partition(dataCar$clm, p = c(train = 0.8, test = 0.2), seed = 9838)\n",
    "\n",
    "y_train <- dataCar$clm[ix$train]\n",
    "X_train <- prep_xgb(dataCar[ix$train, ])\n",
    "\n",
    "# XGBoost data handler\n",
    "dtrain <- xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "# If grid search is to be run again, set tune <- TRUE\n",
    "tune <- FALSE\n",
    "\n",
    "if (tune) {\n",
    "  # Use default parameters to set learning rate with suitable number of rounds\n",
    "  params <- list(\n",
    "    learning_rate = 0.03,\n",
    "    objective = \"binary:logistic\",\n",
    "    eval_metric = \"logloss\"\n",
    "  )\n",
    "  \n",
    "  # Cross-validation\n",
    "  cvm <- xgb.cv(\n",
    "    params = params,\n",
    "    data = dtrain,\n",
    "    nrounds = 5000,\n",
    "    nfold = 5,\n",
    "    early_stopping_rounds = 10,\n",
    "    verbose = 0\n",
    "  )\n",
    "  cvm \n",
    "  \n",
    "  # Grid\n",
    "  grid <- expand.grid(\n",
    "    iteration = NA,\n",
    "    score = NA,\n",
    "    learning_rate = 0.03,\n",
    "    objective = \"binary:logistic\",\n",
    "    eval_metric = \"logloss\",\n",
    "    max_depth = 3:6, \n",
    "    min_child_weight = c(1, 10),\n",
    "    colsample_bytree = c(0.8, 1), \n",
    "    subsample = c(0.8, 1), \n",
    "    reg_lambda = c(0, 2.5, 5, 7.5),\n",
    "    reg_alpha = c(0, 4),\n",
    "    #   tree_method = \"hist\",   # when data is large\n",
    "    min_split_loss = c(0, 1e-04)\n",
    "  )\n",
    "  \n",
    "  # Grid search or randomized search if grid is too large\n",
    "  max_size <- 20\n",
    "  grid_size <- nrow(grid)\n",
    "  if (grid_size > max_size) {\n",
    "    grid <- grid[sample(grid_size, max_size), ]\n",
    "    grid_size <- max_size\n",
    "  }\n",
    "  \n",
    "  # Loop over grid and fit XGBoost with five-fold CV and early stopping\n",
    "  pb <- txtProgressBar(0, grid_size, style = 3)\n",
    "  for (i in seq_len(grid_size)) {\n",
    "    cvm <- xgb.cv(\n",
    "      params = as.list(grid[i, -(1:2)]),\n",
    "      data = dtrain,\n",
    "      nrounds = 5000,\n",
    "      nfold = 5,\n",
    "      early_stopping_rounds = 10,\n",
    "      verbose = 0\n",
    "    )\n",
    "    \n",
    "    # Store result\n",
    "    grid[i, 1] <- cvm$best_iteration\n",
    "    grid[i, 2] <- cvm$evaluation_log[[4]][cvm$best_iteration]\n",
    "    setTxtProgressBar(pb, i)\n",
    "    \n",
    "    # Save grid to survive hard crashs\n",
    "    saveRDS(grid, file = \"r/gridsearch/claims_xgb.rds\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load grid and select best iteration\n",
    "grid <- readRDS(\"r/gridsearch/claims_xgb.rds\")\n",
    "grid <- grid[order(grid$score), ]\n",
    "\n",
    "# Fit final, tuned model\n",
    "fit <- xgb.train(\n",
    "  params = as.list(grid[1, -(1:2)]), \n",
    "  data = dtrain, \n",
    "  nrounds = grid[1, \"iteration\"]\n",
    ")\n",
    "\n",
    "# Interpretation\n",
    "library(MetricsWeighted)\n",
    "library(flashlight)\n",
    "\n",
    "# Performance on test data\n",
    "pred <- predict(fit, prep_xgb(dataCar[ix$test, ]))\n",
    "deviance_bernoulli(dataCar$clm[ix$test], pred)\n",
    "\n",
    "# Relative performance\n",
    "r_squared_bernoulli(dataCar$clm[ix$test], pred) # 0.00427\n",
    "# Relative performance gain is very low, but very slightly better than \n",
    "# with the tuned random forest.\n",
    "\n",
    "# Variable importance regarding loss improvement\n",
    "imp <- xgb.importance(model = fit)\n",
    "xgb.plot.importance(imp)\n",
    "\n",
    "# Partial dependence plots\n",
    "fl <- flashlight(model = fit, \n",
    "                 y = \"clm\", \n",
    "                 data = dataCar[ix$train, ], \n",
    "                 label = \"xgb\", \n",
    "                 predict_function = function(m, X) predict(m, prep_xgb(X)))\n",
    "\n",
    "plot(light_profile(fl, v = \"veh_value\", breaks = seq(0, 5, by = 0.1))) %>% \n",
    "  labs(title = \"Partial dependence plot for veh_value\", y = \"price\")\n",
    "\n",
    "plot(light_profile(fl, v = \"veh_body\"), rotate_x = TRUE) +\n",
    "  labs(title = \"Partial dependence plot for veh_body\", y = \"price\")\n",
    "\n",
    "plot(light_profile(fl, v = \"area\")) +\n",
    "  labs(title = \"Partial dependence plot for area\", y = \"price\")\n",
    "\n",
    "plot(light_profile(fl, v = \"agecat\")) +\n",
    "  labs(title = \"Partial dependence plot for agecat\", y = \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Fit diamond prices by gamma deviance loss with log-link (i.e. exponential \n",
    "output activation), using the custom loss function defined below. Tune the \n",
    "model by simple validation and evaluate it on an independent test data set.\n",
    "Interpret the final model. (Hints: I used a smaller learning rate \n",
    "and had to replace the \"relu\" activations by \"tanh\". Furthermore, the \n",
    "response needed to be transformed from int to float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1805.6743 - val_loss: 757.8267\n",
      "Epoch 2/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 358.9893 - val_loss: 146.4827\n",
      "Epoch 3/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 89.6860 - val_loss: 54.8963\n",
      "Epoch 4/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 39.9848 - val_loss: 29.1300\n",
      "Epoch 5/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 23.0544 - val_loss: 18.2368\n",
      "Epoch 6/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 15.1290 - val_loss: 12.5481\n",
      "Epoch 7/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 10.7384 - val_loss: 9.1916\n",
      "Epoch 8/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 8.0418 - val_loss: 7.0382\n",
      "Epoch 9/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 6.2653 - val_loss: 5.5804\n",
      "Epoch 10/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 5.0356 - val_loss: 4.5483\n",
      "Epoch 11/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 4.1517 - val_loss: 3.7945\n",
      "Epoch 12/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 3.4981 - val_loss: 3.2297\n",
      "Epoch 13/1000\n",
      "108/108 [==============================] - 0s 981us/step - loss: 3.0036 - val_loss: 2.7985\n",
      "Epoch 14/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.6227 - val_loss: 2.4633\n",
      "Epoch 15/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.3250 - val_loss: 2.1995\n",
      "Epoch 16/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 2.0896 - val_loss: 1.9896\n",
      "Epoch 17/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.9014 - val_loss: 1.8212\n",
      "Epoch 18/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.7498 - val_loss: 1.6852\n",
      "Epoch 19/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.6270 - val_loss: 1.5744\n",
      "Epoch 20/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.5267 - val_loss: 1.4838\n",
      "Epoch 21/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.4446 - val_loss: 1.4094\n",
      "Epoch 22/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.3770 - val_loss: 1.3481\n",
      "Epoch 23/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.3212 - val_loss: 1.2973\n",
      "Epoch 24/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.2749 - val_loss: 1.2551\n",
      "Epoch 25/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.2364 - val_loss: 1.2200\n",
      "Epoch 26/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.2043 - val_loss: 1.1906\n",
      "Epoch 27/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1774 - val_loss: 1.1660\n",
      "Epoch 28/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1548 - val_loss: 1.1453\n",
      "Epoch 29/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1358 - val_loss: 1.1277\n",
      "Epoch 30/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1197 - val_loss: 1.1129\n",
      "Epoch 31/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.1060 - val_loss: 1.1002\n",
      "Epoch 32/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0943 - val_loss: 1.0895\n",
      "Epoch 33/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0843 - val_loss: 1.0802\n",
      "Epoch 34/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0758 - val_loss: 1.0722\n",
      "Epoch 35/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0684 - val_loss: 1.0653\n",
      "Epoch 36/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0620 - val_loss: 1.0594\n",
      "Epoch 37/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0565 - val_loss: 1.0542\n",
      "Epoch 38/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0518 - val_loss: 1.0498\n",
      "Epoch 39/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0477 - val_loss: 1.0461\n",
      "Epoch 40/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0442 - val_loss: 1.0429\n",
      "Epoch 41/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0413 - val_loss: 1.0401\n",
      "Epoch 42/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0388 - val_loss: 1.0379\n",
      "Epoch 43/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0367 - val_loss: 1.0360\n",
      "Epoch 44/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0350 - val_loss: 1.0344\n",
      "Epoch 45/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0336 - val_loss: 1.0331\n",
      "Epoch 46/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0324 - val_loss: 1.0321\n",
      "Epoch 47/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0315 - val_loss: 1.0312\n",
      "Epoch 48/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0307 - val_loss: 1.0305\n",
      "Epoch 49/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0301 - val_loss: 1.0300\n",
      "Epoch 50/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0296 - val_loss: 1.0295\n",
      "Epoch 51/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0292 - val_loss: 1.0292\n",
      "Epoch 52/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0288 - val_loss: 1.0289\n",
      "Epoch 53/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0286 - val_loss: 1.0286\n",
      "Epoch 54/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0283 - val_loss: 1.0284\n",
      "Epoch 55/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0281 - val_loss: 1.0281\n",
      "Epoch 56/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0279 - val_loss: 1.0279\n",
      "Epoch 57/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0277 - val_loss: 1.0278\n",
      "Epoch 58/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0275 - val_loss: 1.0276\n",
      "Epoch 59/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0273 - val_loss: 1.0274\n",
      "Epoch 60/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0271 - val_loss: 1.0272\n",
      "Epoch 61/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0270 - val_loss: 1.0270\n",
      "Epoch 62/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0268 - val_loss: 1.0268\n",
      "Epoch 63/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0266 - val_loss: 1.0266\n",
      "Epoch 64/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0263 - val_loss: 1.0264\n",
      "Epoch 65/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0261 - val_loss: 1.0262\n",
      "Epoch 66/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0259 - val_loss: 1.0260\n",
      "Epoch 67/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0257 - val_loss: 1.0257\n",
      "Epoch 68/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0254 - val_loss: 1.0255\n",
      "Epoch 69/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0252 - val_loss: 1.0252\n",
      "Epoch 70/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0249 - val_loss: 1.0250\n",
      "Epoch 71/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0247 - val_loss: 1.0247\n",
      "Epoch 72/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0244 - val_loss: 1.0245\n",
      "Epoch 73/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0241 - val_loss: 1.0242\n",
      "Epoch 74/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0239 - val_loss: 1.0239\n",
      "Epoch 75/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0236 - val_loss: 1.0236\n",
      "Epoch 76/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0233 - val_loss: 1.0233\n",
      "Epoch 77/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0230 - val_loss: 1.0230\n",
      "Epoch 78/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0227 - val_loss: 1.0227\n",
      "Epoch 79/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0224 - val_loss: 1.0224\n",
      "Epoch 80/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0220 - val_loss: 1.0221\n",
      "Epoch 81/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0217 - val_loss: 1.0218\n",
      "Epoch 82/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0214 - val_loss: 1.0214\n",
      "Epoch 83/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0211 - val_loss: 1.0211\n",
      "Epoch 84/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0207 - val_loss: 1.0208\n",
      "Epoch 85/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0204 - val_loss: 1.0204\n",
      "Epoch 86/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0201 - val_loss: 1.0201\n",
      "Epoch 87/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0197 - val_loss: 1.0197\n",
      "Epoch 88/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0194 - val_loss: 1.0194\n",
      "Epoch 89/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0190 - val_loss: 1.0191\n",
      "Epoch 90/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0187 - val_loss: 1.0187\n",
      "Epoch 91/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0183 - val_loss: 1.0183\n",
      "Epoch 92/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0179 - val_loss: 1.0180\n",
      "Epoch 93/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0176 - val_loss: 1.0176\n",
      "Epoch 94/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0172 - val_loss: 1.0173\n",
      "Epoch 95/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0169 - val_loss: 1.0170\n",
      "Epoch 96/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0165 - val_loss: 1.0166\n",
      "Epoch 97/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0162 - val_loss: 1.0163\n",
      "Epoch 98/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0158 - val_loss: 1.0159\n",
      "Epoch 99/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0155 - val_loss: 1.0155\n",
      "Epoch 100/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0151 - val_loss: 1.0151\n",
      "Epoch 101/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0148 - val_loss: 1.0148\n",
      "Epoch 102/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0144 - val_loss: 1.0145\n",
      "Epoch 103/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0141 - val_loss: 1.0141\n",
      "Epoch 104/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0137 - val_loss: 1.0138\n",
      "Epoch 105/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0134 - val_loss: 1.0134\n",
      "Epoch 106/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0130 - val_loss: 1.0131\n",
      "Epoch 107/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0127 - val_loss: 1.0127\n",
      "Epoch 108/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0124 - val_loss: 1.0124\n",
      "Epoch 109/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0121 - val_loss: 1.0122\n",
      "Epoch 110/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0118 - val_loss: 1.0118\n",
      "Epoch 111/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0115 - val_loss: 1.0116\n",
      "Epoch 112/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0113 - val_loss: 1.0113\n",
      "Epoch 113/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0110 - val_loss: 1.0110\n",
      "Epoch 114/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0107 - val_loss: 1.0108\n",
      "Epoch 115/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0105 - val_loss: 1.0105\n",
      "Epoch 116/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0102 - val_loss: 1.0103\n",
      "Epoch 117/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0100 - val_loss: 1.0101\n",
      "Epoch 118/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0098 - val_loss: 1.0099\n",
      "Epoch 119/1000\n",
      "108/108 [==============================] - 0s 991us/step - loss: 1.0096 - val_loss: 1.0096\n",
      "Epoch 120/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0094 - val_loss: 1.0094\n",
      "Epoch 121/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0092 - val_loss: 1.0093\n",
      "Epoch 122/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0091 - val_loss: 1.0092\n",
      "Epoch 123/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0089 - val_loss: 1.0090\n",
      "Epoch 124/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0088 - val_loss: 1.0089\n",
      "Epoch 125/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0087 - val_loss: 1.0087\n",
      "Epoch 126/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0086 - val_loss: 1.0086\n",
      "Epoch 127/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0085 - val_loss: 1.0086\n",
      "Epoch 128/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0084 - val_loss: 1.0085\n",
      "Epoch 129/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0084 - val_loss: 1.0085\n",
      "Epoch 130/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0083 - val_loss: 1.0084\n",
      "Epoch 131/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0083 - val_loss: 1.0085\n",
      "Epoch 132/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0082 - val_loss: 1.0084\n",
      "Epoch 133/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0081 - val_loss: 1.0083\n",
      "Epoch 134/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0081 - val_loss: 1.0083\n",
      "Epoch 135/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0081 - val_loss: 1.0082\n",
      "Epoch 136/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0081 - val_loss: 1.0082\n",
      "Epoch 137/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0080 - val_loss: 1.0081\n",
      "Epoch 138/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0080 - val_loss: 1.0082\n",
      "Epoch 139/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0080 - val_loss: 1.0081\n",
      "Epoch 140/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 1.0080\n",
      "Epoch 141/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 1.0081\n",
      "Epoch 142/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 1.0080\n",
      "Epoch 143/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 1.0080\n",
      "Epoch 144/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0078 - val_loss: 1.0080\n",
      "Epoch 145/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0078 - val_loss: 1.0080\n",
      "Epoch 146/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0078 - val_loss: 1.0080\n",
      "Epoch 147/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0078 - val_loss: 1.0079\n",
      "Epoch 148/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0077 - val_loss: 1.0079\n",
      "Epoch 149/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0077 - val_loss: 1.0078\n",
      "Epoch 150/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0077 - val_loss: 1.0078\n",
      "Epoch 151/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 152/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0079\n",
      "Epoch 153/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 154/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 155/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 156/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 157/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 158/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0076 - val_loss: 1.0078\n",
      "Epoch 159/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 160/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 161/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 162/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 163/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 164/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 165/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 166/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 167/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 168/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 169/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 170/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 171/1000\n",
      "108/108 [==============================] - 0s 1000us/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 172/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 173/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 174/1000\n",
      "108/108 [==============================] - 0s 2ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 175/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 176/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 177/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 178/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 179/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 180/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 181/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.0075 - val_loss: 1.0078\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% deviance explained: 98.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "base": 699.8779907226562,
         "hoverinfo": "text",
         "hoverlabel": {
          "bgcolor": "rgba(0,0,0,0.8)"
         },
         "hovertext": [
          "Model: Functional loss after<br>variable: carat is permuted: 6435.984<br>Drop-out loss change: +5736.106",
          "Model: Functional loss after<br>variable: clarity is permuted: 1849.021<br>Drop-out loss change: +1149.143",
          "Model: Functional loss after<br>variable: color is permuted: 1523.076<br>Drop-out loss change: +823.198",
          "Model: Functional loss after<br>variable: cut is permuted: 749.547<br>Drop-out loss change: +49.669"
         ],
         "marker": {
          "color": "#46bac2"
         },
         "orientation": "h",
         "showlegend": false,
         "text": [
          "+5736.106",
          "+1149.143",
          "+823.198",
          "+49.669"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          5736.1064453125,
          1149.14306640625,
          823.1984252929688,
          49.668701171875
         ],
         "xaxis": "x",
         "y": [
          "carat",
          "clarity",
          "color",
          "cut"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Functional",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "drop-out loss",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0,
          "yanchor": "top",
          "yref": "paper",
          "yshift": -30
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 263,
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 699.8779907226562,
          "x1": 699.8779907226562,
          "xref": "x",
          "y0": -1,
          "y1": 4,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "text": "Variable Importance",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          -160.5379760742187,
          7296.400341796875
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "autorange": "reversed",
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "type": "category"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "customdata": [
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ],
          [
           4403.12841796875,
           "Partial depencence for numeric variables",
           "carat"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>prediction=%{y:.3f}<br>mean_prediction=%{customdata[0]:.3f}<extra></extra>",
         "hovertext": [
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables",
          "Partial depencence for numeric variables"
         ],
         "legendgroup": "Partial depencence for numeric variables",
         "line": {
          "color": "#46bac2",
          "dash": "solid",
          "width": 2
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Partial depencence for numeric variables",
         "opacity": 1,
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.2,
          0.2393,
          0.2786,
          0.31789999999999996,
          0.35719999999999996,
          0.39649999999999996,
          0.43579999999999997,
          0.47509999999999997,
          0.5144,
          0.5537,
          0.593,
          0.6323,
          0.6716,
          0.7108999999999999,
          0.7502,
          0.7894999999999999,
          0.8288,
          0.8680999999999999,
          0.9074,
          0.9466999999999999,
          0.986,
          1.0252999999999999,
          1.0646,
          1.1038999999999999,
          1.1431999999999998,
          1.1824999999999999,
          1.2217999999999998,
          1.2610999999999999,
          1.3003999999999998,
          1.3397,
          1.3789999999999998,
          1.4183,
          1.4575999999999998,
          1.4968999999999997,
          1.5361999999999998,
          1.5754999999999997,
          1.6147999999999998,
          1.6540999999999997,
          1.6933999999999998,
          1.7326999999999997,
          1.7719999999999998,
          1.8112999999999997,
          1.8505999999999998,
          1.8898999999999997,
          1.9291999999999998,
          1.9684999999999997,
          2.0078,
          2.0471,
          2.0864,
          2.1256999999999997,
          2.1649999999999996,
          2.2043,
          2.2436,
          2.2828999999999997,
          2.3222,
          2.3615,
          2.4008,
          2.4400999999999997,
          2.4794,
          2.5187,
          2.558,
          2.5972999999999997,
          2.6366,
          2.6759,
          2.7152,
          2.7544999999999997,
          2.7937999999999996,
          2.8331,
          2.8724,
          2.9116999999999997,
          2.9509999999999996,
          2.9903,
          3.0296,
          3.0688999999999997,
          3.1081999999999996,
          3.1475,
          3.1868,
          3.2260999999999997,
          3.2653999999999996,
          3.3047,
          3.344,
          3.3832999999999998,
          3.4225999999999996,
          3.4618999999999995,
          3.5012,
          3.5404999999999998,
          3.5797999999999996,
          3.6190999999999995,
          3.6584,
          3.6976999999999998,
          3.7369999999999997,
          3.7762999999999995,
          3.8156,
          3.8548999999999998,
          3.8941999999999997,
          3.9334999999999996,
          3.9727999999999994,
          4.012099999999999,
          4.051399999999999,
          4.090699999999999,
          4.13
         ],
         "xaxis": "x",
         "y": [
          345.0491943359375,
          422.4211120605469,
          513.0420532226562,
          618.1785278320312,
          739.0254516601562,
          876.6751708984375,
          1032.089599609375,
          1206.077392578125,
          1399.27685546875,
          1612.1453857421875,
          1844.9552001953125,
          2097.796875,
          2370.583984375,
          2663.068359375,
          2974.85205078125,
          3305.406982421875,
          3654.092041015625,
          4020.172119140625,
          4402.83447265625,
          4801.205078125,
          5214.36328125,
          5641.35400390625,
          6081.1962890625,
          6532.89697265625,
          6995.44677734375,
          7467.83447265625,
          7949.05029296875,
          8438.078125,
          8933.908203125,
          9435.533203125,
          9941.9501953125,
          10452.1572265625,
          10965.1611328125,
          11479.9736328125,
          11995.615234375,
          12511.1123046875,
          13025.5087890625,
          13537.8544921875,
          14047.2294921875,
          14552.72265625,
          15053.4609375,
          15548.59375,
          16037.3134765625,
          16518.857421875,
          16992.4921875,
          17457.5546875,
          17913.41796875,
          18359.533203125,
          18795.39453125,
          19220.568359375,
          19634.6875,
          20037.443359375,
          20428.59375,
          20807.958984375,
          21175.421875,
          21530.919921875,
          21874.4453125,
          22206.044921875,
          22525.80859375,
          22833.8671875,
          23130.392578125,
          23415.58203125,
          23689.671875,
          23952.91796875,
          24205.58984375,
          24447.97265625,
          24680.37890625,
          24903.115234375,
          25116.498046875,
          25320.8359375,
          25516.46484375,
          25703.689453125,
          25882.83203125,
          26054.1953125,
          26218.08984375,
          26374.80078125,
          26524.623046875,
          26667.833984375,
          26804.703125,
          26935.4921875,
          27060.453125,
          27179.8203125,
          27293.830078125,
          27402.701171875,
          27506.64453125,
          27605.8671875,
          27700.552734375,
          27790.892578125,
          27877.0625,
          27959.228515625,
          28037.544921875,
          28112.171875,
          28183.244140625,
          28250.90234375,
          28315.28125,
          28376.49609375,
          28434.67578125,
          28489.9296875,
          28542.3515625,
          28592.064453125,
          28639.14453125
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "carat",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "prediction",
          "textangle": -90,
          "x": -0.07,
          "xref": "paper",
          "y": 0.5,
          "yref": "paper"
         }
        ],
        "font": {
         "color": "#371ea3"
        },
        "height": 489,
        "hovermode": "x unified",
        "legend": {
         "font": {
          "size": 11
         },
         "itemsizing": "constant",
         "orientation": "h",
         "title": {
          "font": {
           "size": 12
          },
          "text": "label"
         },
         "tracegroupgap": 0,
         "x": 1,
         "xanchor": "right",
         "y": 1.0613496932515338,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "text": "Aggregated Profiles",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "showticklabels": true,
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "domain": [
          0,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          -2484.360375976563,
          31468.5541015625
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\ml_lecture\\lib\\site-packages\\dalex\\predict_explanations\\_ceteris_paribus\\checks.py:16: UserWarning:\n",
      "\n",
      "Variables taken from variables_splits\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "modeBarButtonsToRemove": [
         "sendDataToCloud",
         "lasso2d",
         "autoScale2d",
         "select2d",
         "zoom2d",
         "pan2d",
         "zoomIn2d",
         "zoomOut2d",
         "resetScale2d",
         "toggleSpikelines",
         "hoverCompareCartesian",
         "hoverClosestCartesian"
        ],
        "plotlyServerURL": "https://plot.ly",
        "staticPlot": false,
        "toImageButtonOptions": {
         "height": null,
         "width": null
        }
       },
       "data": [
        {
         "alignmentgroup": "True",
         "base": [
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625
         ],
         "customdata": [
          [
           3396.914794921875,
           "Partial depencence for ordinal variables",
           "cut"
          ],
          [
           3596.560302734375,
           "Partial depencence for ordinal variables",
           "cut"
          ],
          [
           3782.396728515625,
           "Partial depencence for ordinal variables",
           "cut"
          ],
          [
           3907.57470703125,
           "Partial depencence for ordinal variables",
           "cut"
          ],
          [
           3958.5732421875,
           "Partial depencence for ordinal variables",
           "cut"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>mean_prediction=%{base:.3f}<br>prediction=%{customdata[0]:.3f}<extra></extra>",
         "hovertext": [
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables"
         ],
         "legendgroup": "Partial depencence for ordinal variables",
         "marker": {
          "color": "#46bac2",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Partial depencence for ordinal variables",
         "offsetgroup": "Partial depencence for ordinal variables",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Fair",
          "Good",
          "Very Good",
          "Premium",
          "Ideal"
         ],
         "xaxis": "x3",
         "y": [
          -464.318115234375,
          -264.672607421875,
          -78.836181640625,
          46.341796875,
          97.34033203125
         ],
         "yaxis": "y3"
        },
        {
         "alignmentgroup": "True",
         "base": [
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625
         ],
         "customdata": [
          [
           4732.12744140625,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           4538.02099609375,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           4262.65380859375,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           3926.381591796875,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           3549.85498046875,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           3149.998291015625,
           "Partial depencence for ordinal variables",
           "color"
          ],
          [
           2739.9462890625,
           "Partial depencence for ordinal variables",
           "color"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>mean_prediction=%{base:.3f}<br>prediction=%{customdata[0]:.3f}<extra></extra>",
         "hovertext": [
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables"
         ],
         "legendgroup": "Partial depencence for ordinal variables",
         "marker": {
          "color": "#46bac2",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Partial depencence for ordinal variables",
         "offsetgroup": "Partial depencence for ordinal variables",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "D",
          "E",
          "F",
          "G",
          "H",
          "I",
          "J"
         ],
         "xaxis": "x4",
         "y": [
          870.89453125,
          676.7880859375,
          401.4208984375,
          65.148681640625,
          -311.3779296875,
          -711.234619140625,
          -1121.28662109375
         ],
         "yaxis": "y4"
        },
        {
         "alignmentgroup": "True",
         "base": [
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625,
          3861.23291015625
         ],
         "customdata": [
          [
           2337.655517578125,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           2928.00244140625,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           3519.085205078125,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           4086.258544921875,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           4613.9443359375,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           5080.8369140625,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           5460.5771484375,
           "Partial depencence for ordinal variables",
           "clarity"
          ],
          [
           5739.5732421875,
           "Partial depencence for ordinal variables",
           "clarity"
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>mean_prediction=%{base:.3f}<br>prediction=%{customdata[0]:.3f}<extra></extra>",
         "hovertext": [
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables",
          "Partial depencence for ordinal variables"
         ],
         "legendgroup": "Partial depencence for ordinal variables",
         "marker": {
          "color": "#46bac2",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Partial depencence for ordinal variables",
         "offsetgroup": "Partial depencence for ordinal variables",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "I1",
          "SI2",
          "SI1",
          "VS2",
          "VS1",
          "VVS2",
          "VVS1",
          "IF"
         ],
         "xaxis": "x",
         "y": [
          -1523.577392578125,
          -933.23046875,
          -342.147705078125,
          225.025634765625,
          752.71142578125,
          1219.60400390625,
          1599.34423828125,
          1878.34033203125
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "clarity",
          "x": 0.2375,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.425,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "cut",
          "x": 0.2375,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "color",
          "x": 0.7625,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 13
          },
          "showarrow": false,
          "text": "prediction",
          "textangle": -90,
          "x": -0.07,
          "xref": "paper",
          "y": 0.5,
          "yref": "paper"
         }
        ],
        "barmode": "group",
        "font": {
         "color": "#371ea3"
        },
        "height": 829,
        "hovermode": "x unified",
        "legend": {
         "font": {
          "size": 11
         },
         "itemsizing": "constant",
         "orientation": "h",
         "title": {
          "font": {
           "size": 12
          },
          "text": "label"
         },
         "tracegroupgap": 0,
         "x": 1,
         "xanchor": "right",
         "y": 1.0361881785283473,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 71,
         "r": 30,
         "t": 78
        },
        "shapes": [
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x3 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y3"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x4 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y4"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x3 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y3"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x4 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y4"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x3 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y3"
         },
         {
          "layer": "below",
          "line": {
           "color": "#371ea3",
           "dash": "dot",
           "width": 1.5
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x4 domain",
          "y0": 3861.23291015625,
          "y1": 3861.23291015625,
          "yref": "y4"
         }
        ],
        "template": {
         "data": {
          "scatter": [
           {
            "type": "scatter"
           }
          ]
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "text": "Aggregated Profiles",
         "x": 0.15
        },
        "xaxis": {
         "anchor": "y",
         "automargin": true,
         "domain": [
          0,
          0.475
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "showticklabels": true,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "category"
        },
        "xaxis2": {
         "anchor": "y2",
         "automargin": true,
         "domain": [
          0.525,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "showticklabels": true,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "category"
        },
        "xaxis3": {
         "anchor": "y3",
         "automargin": true,
         "domain": [
          0,
          0.475
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "showticklabels": true,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "category"
        },
        "xaxis4": {
         "anchor": "y4",
         "automargin": true,
         "domain": [
          0.525,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "showticklabels": true,
         "tickcolor": "white",
         "ticklen": 10,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "category"
        },
        "yaxis": {
         "anchor": "x",
         "automargin": true,
         "domain": [
          0,
          0.425
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "range": [
          1997.4637451171875,
          6079.7650146484375
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        },
        "yaxis2": {
         "anchor": "x2",
         "automargin": true,
         "domain": [
          0,
          0.425
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "matches": "y",
         "range": [
          1997.4637451171875,
          6079.7650146484375
         ],
         "showticklabels": false,
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        },
        "yaxis3": {
         "anchor": "x3",
         "automargin": true,
         "domain": [
          0.575,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "matches": "y",
         "range": [
          1997.4637451171875,
          6079.7650146484375
         ],
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        },
        "yaxis4": {
         "anchor": "x4",
         "automargin": true,
         "domain": [
          0.575,
          1
         ],
         "fixedrange": true,
         "gridwidth": 2,
         "matches": "y",
         "range": [
          1997.4637451171875,
          6079.7650146484375
         ],
         "showticklabels": false,
         "tickcolor": "white",
         "ticklen": 3,
         "ticks": "outside",
         "title": {
          "text": ""
         },
         "type": "linear",
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data and specify preprocessing\n",
    "from plotnine.data import diamonds\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diamonds[\"price\"] = diamonds[\"price\"].astype(\"float32\")  # for TensorFlow\n",
    "\n",
    "ord_features = [\"cut\", \"color\", \"clarity\"]\n",
    "ord_levels = [\n",
    "    diamonds[x].cat.categories.to_list() for x in ord_features\n",
    "]\n",
    "\n",
    "preprocessor = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"ordinal\", OrdinalEncoder(categories=ord_levels), ord_features),\n",
    "            (\"numeric\", \"passthrough\", [\"carat\"])\n",
    "        ]\n",
    "    ),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "df_train, df_valid, y_train, y_valid = train_test_split(\n",
    "    diamonds, diamonds[\"price\"], test_size=0.2, random_state=341\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(df_train)\n",
    "X_valid = preprocessor.transform(df_valid)\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError as RMSE\n",
    "import keras.backend as K\n",
    "\n",
    "def loss_gamma(y_true, y_pred):\n",
    "  return -K.log(y_true / y_pred) + y_true / y_pred\n",
    "\n",
    "inputs = keras.Input(shape=4)\n",
    "x = layers.Dense(30, activation=\"tanh\")(inputs)\n",
    "x = layers.Dense(15, activation=\"tanh\")(x)\n",
    "outputs = layers.Dense(1, activation=K.exp)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss=loss_gamma,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "\n",
    "cb = [\n",
    "    keras.callbacks.EarlyStopping(patience=20),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "]\n",
    "\n",
    "tf.random.set_seed(88)\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=400, \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training RMSE over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.gca().set(\n",
    "    title=\"Training Gamma loss over epochs\",\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel=\"Gamma loss\",\n",
    "    ylim=(0, 3)\n",
    ")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "from sklearn.metrics import mean_gamma_deviance as deviance\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import dalex as dx\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode()  # for saving html with plotly plots\n",
    "\n",
    "dummy = DummyRegressor().fit(X_train, y_train)\n",
    "d0, d = (deviance(y_valid, m.predict(X_valid)) for m in (dummy, model))\n",
    "print(f\"% deviance explained: {(d0 - d) / d0:.2%}\")\n",
    "\n",
    "def pred_fun(m, X):\n",
    "    return m.predict(preprocessor.transform(X), batch_size=1000).flatten()\n",
    "\n",
    "exp = dx.Explainer(\n",
    "    model, \n",
    "    data=df_valid[ord_features + [\"carat\"]], \n",
    "    y=y_valid, \n",
    "    predict_function=pred_fun, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "vi = exp.model_parts()\n",
    "vi.plot()\n",
    "\n",
    "pdp_num = exp.model_profile(\n",
    "    type=\"partial\",\n",
    "    label=\"Partial depencence for numeric variables\",\n",
    "    variables=[\"carat\"],\n",
    "    verbose=False\n",
    ")\n",
    "pdp_num.plot()\n",
    "\n",
    "pdp_ord = exp.model_profile(\n",
    "    type=\"partial\",\n",
    "    label=\"Partial depencence for ordinal variables\",\n",
    "    variables=ord_features,\n",
    "    variable_type=\"categorical\",\n",
    "    variable_splits=dict(zip(ord_features, ord_levels)),\n",
    "    verbose=False\n",
    ")\n",
    "pdp_ord.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Study either the optional claims data example or build your own neural net, \n",
    "predicting claim yes/no. For simplicity, you can represent the categorical \n",
    "feature `veh_body` by integers.\n",
    "\n",
    "-> see lecture notes for a solution with embeddings"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "813f6c4cc22c03b594219174db46929469d46b04ef4db7b355912617e499b8c1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ml_lecture': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
