{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\r\n",
    "\r\n",
    "In this chapter, we dive into artificial neural networks, one of the main drivers of artificial intelligence. \r\n",
    "\r\n",
    "Neural networks are around since many decades. (Maybe) the first such model was built by Marvin Minsky in 1951. He called his algorithm SNARC (\"stochastic neural-analog reinforcement calculator\"). Since then, neural networks have gone through several stages of development. One of the milestones was the idea of Paul J. Werbos in 1974 [1] to efficiently calculate gradients in the optimization algorithm by an approach called \"backpropagation\". Another milestone was the use of GPUs  (graphics processing units) to greatly reduce calculation time.\r\n",
    "\r\n",
    "Artificial neural nets are extremely versatile and powerful. They can be used to\r\n",
    "\r\n",
    "1. fit simple models like GLMs,\r\n",
    "2. learn interactions and non-linear effects in an automatic way (like tree-based methods),\r\n",
    "3. optimize general loss functions,\r\n",
    "4. fit data much larger than RAM (e.g. images),\r\n",
    "5. learn \"online\" (update the model with additional data),\r\n",
    "6. fit multiple response variables at the same time,\r\n",
    "7. model input of dimension higher than two (e.g. images, videos),\r\n",
    "8. model input of *different* input dimensions (e.g. text *and* images),\r\n",
    "9. fit data with sequential structure in both in- and output (e.g. a text translator),\r\n",
    "10. model data with spatial structure (images),\r\n",
    "11. fit models with many millions of parameters,\r\n",
    "12. do non-linear dimension reduction.\r\n",
    "\r\n",
    "In this chapter, we will mainly deal with the first three aspects. Since a lot of new terms are being used, a small glossary can be found in Section \"Neural Network Slang\".\r\n",
    "\r\n",
    "# Understanding Neural Nets\r\n",
    "\r\n",
    "To learn how and why neural networks work, we will go through three steps - each illustrated on the diamonds data:\r\n",
    "\r\n",
    "- Step 1: Linear regression as neural net\r\n",
    "- Step 2: Hidden layers\r\n",
    "- Step 3: Activation functions\r\n",
    "\r\n",
    "After this, we will be ready to build more complex models.\r\n",
    "\r\n",
    "## Step 1: Linear regression as neural net\r\n",
    "\r\n",
    "Let us revisit the simple linear regression\r\n",
    "$$\r\n",
    "  E(\\text{price}) = \\alpha + \\beta \\cdot \\text{carat}\r\n",
    "$$\r\n",
    "calculated on the full diamonds data. In Chapter 1 we have found the solution $\\hat\\alpha = -2256.36$ and $\\hat \\beta = 7756.43$ by ordinary least-squares.\r\n",
    "\r\n",
    "Above situation can be viewed as a neural network with\r\n",
    "\r\n",
    "- an input layer with two nodes (`carat` and the intercept called \"bias unit\" with value 1),\r\n",
    "- a \"fully connected\" (= \"dense\") output layer with one node (`price`). Fully connected means that each node of a layer is a linear function of all node values of the previous layer. Each linear function has parameters or *weights* to be estimated, in our simple case just $\\alpha$ and $\\beta$.\r\n",
    "\r\n",
    "Visualized as a graph, the situation looks as follows.\r\n",
    "\r\n",
    "![](../figs/nn_simple_linear.PNG)\r\n",
    "\r\n",
    "*Part of the figures were done with this cool [webtool](http://alexlenail.me/NN-SVG/index.html).*\r\n",
    "\r\n",
    "To gain confidence in neural nets, we first show that parameters estimated by a neural network are quite similar to the ones learned by linear least-squares. To do so, we will use Google's [TensorFlow](https://www.tensorflow.org/) with its convenient (functional) [Keras](https://keras.io/) interface. \r\n",
    "\r\n",
    "### Example: simple linear regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError as RMSE\r\n",
    "from plotnine.data import diamonds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Input layer: we have 1 covariate\r\n",
    "inputs = keras.Input(shape=(1,))\r\n",
    "\r\n",
    "# Output layer densely connected to the input layer\r\n",
    "dense = layers.Dense(1)\r\n",
    "output = dense(inputs)\r\n",
    "\r\n",
    "# Create model\r\n",
    "nn = keras.Model(inputs=inputs, outputs=output)\r\n",
    "# nn.summary()\r\n",
    "\r\n",
    "# Compile model\r\n",
    "nn.compile(\r\n",
    "    loss=\"mse\",\r\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1),\r\n",
    "    metrics=[RMSE()],\r\n",
    ")\r\n",
    "\r\n",
    "# Fit model - naive without validation\r\n",
    "history = nn.fit(\r\n",
    "  x = diamonds[\"carat\"],\r\n",
    "  y = diamonds[\"price\"],\r\n",
    "  epochs = 2,\r\n",
    "  batch_size = 64\r\n",
    ")\r\n",
    "# Plot effect of carat on average price\r\n",
    "#data.frame(carat = seq(0.3, 3, by = 0.1)) %>% \r\n",
    "#  mutate(price = predict(nn, carat)) %>% \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "843/843 [==============================] - 2s 2ms/step - loss: 25222254.0000 - root_mean_squared_error: 5022.1763\n",
      "Epoch 2/2\n",
      "843/843 [==============================] - 2s 2ms/step - loss: 16375297.0000 - root_mean_squared_error: 4046.6401\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "plt.plot(history.history[\"root_mean_squared_error\"])\r\n",
    "plt.ylabel(\"RMSE\")\r\n",
    "plt.xlabel(\"Epoch\")\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "nn.get_weights()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[1434.7789]], dtype=float32), array([1349.4714], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comment:** The solution of the simple neural network is indeed quite similar to the OLS solution. \r\n",
    "\r\n",
    "### The optimization algorithm\r\n",
    "\r\n",
    "Neural nets are typically fitted by **mini-batch gradient descent**, using **backpropagation** to efficiently calculate gradients. It works as follows:\r\n",
    "\r\n",
    "1. Initiate the parameters with random values. \r\n",
    "2. Forward step: Use the parameters to predict all observations of a *batch*. A batch is a randomly selected subset of the full data set.\r\n",
    "3. Backpropagation step: Change the parameters in the right direction, making the loss of the current batch smaller. This involves calculating derivatives (\"gradients\") of the loss function (e.g. MSE) with respect to all parameters. Backpropagation does so in a layer-per-layer fashion, making heavy use of the chain rule. \r\n",
    "4. Repeat Steps 2-3 until each observation appeared in a batch. This is called an *epoch*.\r\n",
    "5. Repeat Step 4 for multiple epochs until the parameter estimates stabilize or validation performance stops improving.\r\n",
    "\r\n",
    "Gradient descent on batches of size 1 is called \"stochastic gradient descent\" (SGD).\r\n",
    "\r\n",
    "## Step 2: Hidden layers\r\n",
    "\r\n",
    "Our first neural network above consisted of only an input layer and an output layer. By adding one or more *hidden* layers between in- and output, the network gains additional parameters, i.e. more flexibility. The nodes of a hidden layer can be viewed as latent variables, representing the original covariates. The nodes of a hidden layer are sometimes called *encoding*. The closer a layer is to the output, the better its nodes are suitable to predict the response variable. In this way, a neural network finds the right transformations and interactions of its covariates in an automatic way. The only ingredients are a large data set and a flexible enough network \"architecture\" (number of layers, nodes per layer). \r\n",
    "\r\n",
    "Neural nets with more than one hidden layer are called \"deep neural nets\".\r\n",
    "\r\n",
    "We will now add a hidden layer with five nodes $v_1, \\dots, v_5$ to our simple linear regression network. The architecture looks as follows:\r\n",
    "\r\n",
    "![](../figs/nn_1_hidden.PNG)\r\n",
    "\r\n",
    "This network has 16 parameters. How much better than our simple network with just two parameters will it be?\r\n",
    "\r\n",
    "### Example: hidden layer\r\n",
    "\r\n",
    "The following code is identical to the last one up to one extra line of code specifying the hidden layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "library(tidyverse)\r\n",
    "library(keras)\r\n",
    "# use_python(path to python...)\r\n",
    "\r\n",
    "# RMSE metric needs to be defined \"by hand\"\r\n",
    "metric_rmse <- custom_metric(\"rmse\", function(y_true, y_pred) {\r\n",
    "  sqrt(k_mean(k_square(y_true - y_pred)))\r\n",
    "})\r\n",
    "\r\n",
    "# Input layer: we have 1 covariate\r\n",
    "input <- layer_input(shape = 1)\r\n",
    "\r\n",
    "# One hidden layer\r\n",
    "output <- input %>%\r\n",
    "  layer_dense(units = 5) %>%  # the only new line of code!\r\n",
    "  layer_dense(units = 1)\r\n",
    "\r\n",
    "# Create and compile model\r\n",
    "nn <- keras_model(inputs = input, outputs = output)\r\n",
    "# summary(nn)\r\n",
    "nn %>% compile(\r\n",
    "  optimizer = optimizer_adam(lr = 1),\r\n",
    "  loss = 'mse',\r\n",
    "  metrics = metric_rmse\r\n",
    ")\r\n",
    "\r\n",
    "# Fit model - naive without validation\r\n",
    "nn %>% fit(\r\n",
    "  x = diamonds$carat,\r\n",
    "  y = diamonds$price,\r\n",
    "  epochs = 30,\r\n",
    "  batch_size = 100\r\n",
    ")\r\n",
    "\r\n",
    "# Plot effect of carat on average price\r\n",
    "data.frame(carat = seq(0.3, 3, by = 0.1)) %>% \r\n",
    "  mutate(price = predict(nn, carat)) %>% \r\n",
    "ggplot(aes(x = carat, y = price)) +\r\n",
    "  geom_line() +\r\n",
    "  geom_point()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comment:** Oops, it seems as if the extra hidden layer had no effect. The reason is that a linear function of a linear function is still a linear function. Adding the hidden layer did not really change the capabilities of the model. It just added a lot of unnecessary parameters.\r\n",
    "\r\n",
    "## Step 3: Activation functions\r\n",
    "\r\n",
    "The missing magic component is the so called [*activation* function](https://en.wikipedia.org/wiki/Activation_function) $\\sigma$ after each layer, which transforms the values of the nodes. So far, we have implicitly used \"linear activations\", which - in neural network slang - is just the identity function. \r\n",
    "\r\n",
    "Applying *non-linear* activation functions after hidden layers have the purpose to introduce non-linear and interaction effects. Typical such functions are\r\n",
    "\r\n",
    "- the hyperbolic tangent (\"S\"-shaped function that maps real values to $[-1, 1]$),\r\n",
    "- the sigmoidal function (\"S\"-shaped function that maps real values to $[0, 1]$),\r\n",
    "- the **re**ctangular **l**inear **u**nit \"ReLU\" $f(x) = \\text{max}(0, x)$ that sets negative values to 0.\r\n",
    "\r\n",
    "Activation functions applied to the *output* layer have a different purpose, namely the same as the inverse of the link function of a corresponding GLM. It maps predictions to the scale of the response: \r\n",
    "\r\n",
    "- linear regression -> linear activation\r\n",
    "- binary logistic regression -> sigmoid activation (to predict probability of \"1\")\r\n",
    "- multinomial logistic regression -> softmax activation (to predict one probability per class)\r\n",
    "- log-linear regression -> exponential activation\r\n",
    "\r\n",
    "Let us add a hyperbolic tangent activation function ($\\sigma$) after the hidden layer of our simple example.\r\n",
    "\r\n",
    "![](../figs/nn_activation.PNG)\r\n",
    "\r\n",
    "### Example: activation functions\r\n",
    "\r\n",
    "Again, the code is very similar to the last one, with the exception of using a hyperbolic tangent activation after the hidden layer (and different learning rate and number of epochs)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "library(tidyverse)\r\n",
    "library(keras)\r\n",
    "# use_python(path to python...)\r\n",
    "\r\n",
    "# RMSE metric needs to be defined \"by hand\"\r\n",
    "metric_rmse <- custom_metric(\"rmse\", function(y_true, y_pred) {\r\n",
    "  sqrt(k_mean(k_square(y_true - y_pred)))\r\n",
    "})\r\n",
    "\r\n",
    "# Input layer: we have 1 covariate\r\n",
    "input <- layer_input(shape = 1)\r\n",
    "\r\n",
    "# One hidden layer\r\n",
    "output <- input %>%\r\n",
    "  layer_dense(units = 5, activation = 'tanh') %>% \r\n",
    "  layer_dense(units = 1, activation = 'linear')\r\n",
    "\r\n",
    "# Create and compile model\r\n",
    "nn <- keras_model(inputs = input, outputs = output)\r\n",
    "\r\n",
    "nn %>% compile(\r\n",
    "  optimizer = optimizer_adam(lr = 0.2),\r\n",
    "  loss = 'mse',\r\n",
    "  metrics = metric_rmse\r\n",
    ")\r\n",
    "\r\n",
    "# Fit model - naive without validation\r\n",
    "nn %>% fit(\r\n",
    "  x = diamonds$carat,\r\n",
    "  y = diamonds$price,\r\n",
    "  epochs = 50,\r\n",
    "  batch_size = 100\r\n",
    ")\r\n",
    "\r\n",
    "# Plot effect of carat on average price\r\n",
    "data.frame(carat = seq(0.3, 3, by = 0.1)) %>% \r\n",
    "  mutate(price = predict(nn, carat)) %>% \r\n",
    "ggplot(aes(x = carat, y = price)) +\r\n",
    "  geom_line() +\r\n",
    "  geom_point()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comment:** Adding the non-linear activation after the hidden layer has changed the model. The effect of carat is now representing the association between carat and price by a non-linear function.\r\n",
    "\r\n",
    "# Practical Considerations\r\n",
    "\r\n",
    "## Validation and tuning of main parameters\r\n",
    "\r\n",
    "So far, we have naively fitted the neural networks without splitting the data for test and validation. Don't do this! Usually, one sets a small test dataset (e.g. 10% of rows) aside to assess the final model performance and use simple (or cross-)validation for model tuning.\r\n",
    "\r\n",
    "In order to choose the main tuning parameters, namely\r\n",
    "\r\n",
    "- network architecture,\r\n",
    "- activation functions,\r\n",
    "- learning rate, \r\n",
    "- batch size, and\r\n",
    "- number of epochs, \r\n",
    "\r\n",
    "one often uses simple validation because cross-validation takes too much time.\r\n",
    "\r\n",
    "## Missing values\r\n",
    "\r\n",
    "A neural net does not accept missing values in the input. They need to be filled, e.g. by a typcial value or a value below the minimum.\r\n",
    "\r\n",
    "## Input standardization\r\n",
    "\r\n",
    "Gradient descent starts by random initialization of parameters. This step is optimized for standardized input. Standardization has to be done manually by either\r\n",
    "\r\n",
    "- min/max scale the values of each input to the range -1 to 1,\r\n",
    "- standard scale the values of each input to mean 0 and standard deviation 1, or\r\n",
    "- use relative ranks.\r\n",
    "\r\n",
    "Note that the scaling transformation is calculated on the training data and then applied to the validation and test data. This usually requires a couple of lines of code.\r\n",
    "\r\n",
    "## Categorical input\r\n",
    "\r\n",
    "There are three ways to represent categorical input variables in a neural network. \r\n",
    "\r\n",
    "1. Binary and ordinal categoricals are best represented by integers and then treated as numeric.\r\n",
    "2. Unordered categoricals are either one-hot-encoded (i.e. each category is represented by a binary variable) or \r\n",
    "3. they are represented by a (categorical) embedding. To do so, the categories are integer encoded and then condensed by a special *embedding layer* to a few (usually 1 or 2) dense features. This requires a more complex network architecture but saves memory and preprocessing. This approach is heavily used when the input consists of words (which is a categorical variable with thousands of levels - one level per word).\r\n",
    "\r\n",
    "For Option 2, input standardization is not required, for Option 3 it *must* not be applied as the embedding layer expects integers.\r\n",
    "\r\n",
    "## Callbacks\r\n",
    "\r\n",
    "Sometimes, we want to take action during training, e.g.\r\n",
    "\r\n",
    "- stop training when validation performance starts worsening,\r\n",
    "- reduce the learning rate when the optimization is stuck in a \"plateau\", or\r\n",
    "- save the network weights between epochs.\r\n",
    "\r\n",
    "Such monitoring tasks are called *callbacks*. We will see them in the example below.\r\n",
    "\r\n",
    "## Types of layers\r\n",
    "\r\n",
    "So far, we have encountered only dense (= fully connected) layers and activation layers. Here some further types:\r\n",
    "\r\n",
    "- Embedding layers to represent integer encoded categoricals.\r\n",
    "- Dropout layers to add regularization.\r\n",
    "- Convolutional and pooling layers for image data.\r\n",
    "- Recurrent layers (long-short-term memory LSTM, gated recurrent unit GRU) for sequence data.\r\n",
    "- Concatenation layers to combine different branches of the network (like in a directed graph).\r\n",
    "- Flatten layers to bring higher dimensional layers to dimension 1 (e.g. for embeddings, image and text data).\r\n",
    "\r\n",
    "## Optimizer\r\n",
    "\r\n",
    "Pure gradient descent is rarely applied without tweaks because it tends to be stuck in local minima, especially for complex networks with non-convex objective surfaces. Modern variants are \"adam\", \"nadam\" and \"RMSProp\". These optimizers work usually out-of-the-box, except for the learning rate, which has to be manually chosen.\r\n",
    "\r\n",
    "## Custom losses and evaluation metrics\r\n",
    "\r\n",
    "Frameworks like Keras/TensorFlow offer many predefined loss functions and evaluation metrics. Choosing them is a crucial step, just as with tree boosting.\r\n",
    "Using TensorFlow's backend functions, one can define own metrics (see example above for the root-mean-squared error) and loss functions (see exercises).\r\n",
    "\r\n",
    "## Overfitting and regularization\r\n",
    "\r\n",
    "Like with linear models, a model with too many parameters will overfit in an undesired way. With about 50 to 100 observations per parameter, overfitting is usually unproblematic. (For image and text data, different rules apply). As with penalized regression or trees, there are ways to actively reduce effects of overfitting in neural nets. The two main options are\r\n",
    "\r\n",
    "- pull the parameters of a layer slightly towards zero by applying L1 and/or L2 penalties to the parameters,\r\n",
    "- add dropout layers. A dropout layer randomly sets some of the node values of the previous layer to 0, switching them off. This is an elegant way to fight overfitting and is related to bagging.\r\n",
    "\r\n",
    "## Choosing the architecture\r\n",
    "\r\n",
    "How many layers and number of nodes per layer to select? For tabular data, using 1-3 hidden layers is usually enough. If we start with $m$ input variables, the number of nodes in the first hidden layer is usually higher than $m$ and reduces for later layers. There should not be a \"representational bottleneck\", i.e. an early hidden layer with too few parameters. \r\n",
    "\r\n",
    "The number of parameters should not be too high compared to the number of rows, see \"Overfitting and regularization\" above. \r\n",
    "\r\n",
    "## Interpretation\r\n",
    "\r\n",
    "Variable importance of covariates in neural networks can be assessed by permutation importance (how much performance is lost when shuffling column X?) or SHAP importance. covariate effects can be investigated e.g. by partial dependence plots or SHAP dependence plots.\r\n",
    "\r\n",
    "# Example: diamonds\r\n",
    "\r\n",
    "We will now fit a neural net with two hidden layers (30 and 15 nodes) and a total of 631 parameters to model diamond prices. Learning rate and batch size were chosen by simple validation. The number of epochs is being chosen by an early stopping callback.\r\n",
    "\r\n",
    "![](../figs/nn_2_hidden.PNG)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "library(tidyverse)\r\n",
    "library(splitTools)\r\n",
    "library(keras)\r\n",
    "# use_python(path to python...)\r\n",
    "\r\n",
    "# RMSE metric needs to be defined \"by hand\"\r\n",
    "metric_rmse <- custom_metric(\"rmse\", function(y_true, y_pred) {\r\n",
    "  sqrt(k_mean(k_square(y_true - y_pred)))\r\n",
    "})\r\n",
    "\r\n",
    "# Response and covariates\r\n",
    "y <- \"price\"\r\n",
    "x <- c(\"carat\", \"color\", \"cut\", \"clarity\")\r\n",
    "\r\n",
    "# Split into train and validation\r\n",
    "ix <- partition(diamonds[[y]], p = c(train = 0.8, valid = 0.2), seed = 9838)\r\n",
    "\r\n",
    "train <- diamonds[ix$train, ]\r\n",
    "valid <- diamonds[ix$valid, ]\r\n",
    "\r\n",
    "y_train <- as.numeric(train[[y]])  # integers are not auto-casted by all tf versions\r\n",
    "y_valid <- as.numeric(valid[[y]])\r\n",
    "\r\n",
    "X_train <- train[, x]\r\n",
    "X_valid <- valid[, x]\r\n",
    "\r\n",
    "# Standardize X using X_train\r\n",
    "sc <- list(\r\n",
    "  center = attr(scale(data.matrix(X_train)), \"scaled:center\"),\r\n",
    "  scale = attr(scale(data.matrix(X_train)), \"scaled:scale\")\r\n",
    ")\r\n",
    "\r\n",
    "# Function that maps data to scaled network input\r\n",
    "prep_nn <- function(X, x = c(\"carat\", \"color\", \"cut\", \"clarity\"), scaling = sc) {\r\n",
    "  X <- data.matrix(X[, x, drop = FALSE])\r\n",
    "  scale(X, center = scaling$center, scale = scaling$scale)\r\n",
    "}\r\n",
    "\r\n",
    "# Input layer: we have 4 covariates\r\n",
    "input <- layer_input(shape = 4)\r\n",
    "\r\n",
    "# Two hidden layers with contracting number of nodes\r\n",
    "output <- input %>%\r\n",
    "  layer_dense(units = 30, activation = 'relu') %>% \r\n",
    "  layer_dense(units = 15, activation = 'relu') %>% \r\n",
    "  layer_dense(units = 1, activation = 'linear')\r\n",
    "\r\n",
    "# Create and compile model\r\n",
    "nn <- keras_model(inputs = input, outputs = output)\r\n",
    "summary(nn)\r\n",
    "nn %>% compile(\r\n",
    "  optimizer = optimizer_adam(lr = 0.3),\r\n",
    "  loss = 'mse',\r\n",
    "  metrics = metric_rmse\r\n",
    ")\r\n",
    "\r\n",
    "# Callbacks\r\n",
    "cb <- list(\r\n",
    "  callback_early_stopping(patience = 20),\r\n",
    "  callback_reduce_lr_on_plateau(patience = 5)\r\n",
    ")\r\n",
    "       \r\n",
    "# Fit model\r\n",
    "history <- nn %>% fit(\r\n",
    "  x = prep_nn(X_train),\r\n",
    "  y = y_train,\r\n",
    "  epochs = 100,\r\n",
    "  batch_size = 400, \r\n",
    "  validation_data = list(prep_nn(X_valid), y_valid),\r\n",
    "  callbacks = cb\r\n",
    ")\r\n",
    "\r\n",
    "history$metrics[c(\"rmse\", \"val_rmse\")] %>% \r\n",
    "  data.frame() %>% \r\n",
    "  mutate(epoch = row_number()) %>% \r\n",
    "  pivot_longer(cols = c(\"rmse\", \"val_rmse\")) %>% \r\n",
    "ggplot(aes(x = epoch, y = value, group = name, color = name)) +\r\n",
    "  geom_line(size = 1.4)\r\n",
    "\r\n",
    "# Interpret\r\n",
    "library(flashlight)\r\n",
    "library(MetricsWeighted)\r\n",
    "\r\n",
    "fl <- flashlight(\r\n",
    "  model = nn, \r\n",
    "  y = \"price\", \r\n",
    "  data = diamonds[ix$valid, ], \r\n",
    "  label = \"nn\", \r\n",
    "  metrics = list(rmse = rmse, `R squared` = r_squared),\r\n",
    "  predict_function = function(m, X) predict(m, prep_nn(X), batch_size = 1000)\r\n",
    ")\r\n",
    "\r\n",
    "# Performance on validation data\r\n",
    "plot(light_performance(fl), fill = \"orange\")\r\n",
    "\r\n",
    "# Permutation importance\r\n",
    "plot(light_importance(fl, v = x), fill = \"orange\")\r\n",
    "\r\n",
    "# Partial dependence plots\r\n",
    "plot(light_profile(fl, v = \"carat\", n_bins = 40)) +\r\n",
    "  labs(title = \"Partial dependence plot for carat\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"clarity\")) +\r\n",
    "  labs(title = \"Partial dependence plot for clarity\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"cut\")) +\r\n",
    "  labs(title = \"Partial dependence plot for cut\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"color\")) +\r\n",
    "  labs(title = \"Partial dependence plot for color\", y = \"price\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comments** \r\n",
    "\r\n",
    "- The model performance seems comparable to the tree-based models from the last chapter. The specific performance might change from run to run due to randomness in the algorithm.\r\n",
    "- The effect of `carat` looks smoother as with random forests or boosted trees.\r\n",
    "\r\n",
    "# Example: Embeddings (optional)\r\n",
    "\r\n",
    "Representing categorical input variables through embedding layers is extremely useful in practice. We will end this chapter with an example on how to do it with the claims data. This example also shows how flexible neural network structures are."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "library(tidyverse)\r\n",
    "library(keras)\r\n",
    "library(splitTools)\r\n",
    "library(insuranceData)\r\n",
    "data(dataCar)\r\n",
    "\r\n",
    "# use_python(path to python...)\r\n",
    "\r\n",
    "# Response and covariates\r\n",
    "y <- \"clm\"\r\n",
    "x_emb <- \"veh_body\"\r\n",
    "x_dense <- c(\"veh_value\", \"veh_age\", \"gender\", \"area\", \"agecat\")\r\n",
    "x <- c(x_dense, x_emb)\r\n",
    "\r\n",
    "# Split into train and validation\r\n",
    "ix <- partition(dataCar[[y]], p = c(train = 0.8, valid = 0.2), seed = 9838)\r\n",
    "\r\n",
    "train <- dataCar[ix$train, ]\r\n",
    "valid <- dataCar[ix$valid, ]\r\n",
    "\r\n",
    "y_train <- train[[y]]\r\n",
    "y_valid <- valid[[y]]\r\n",
    "\r\n",
    "X_train <- train[, x]\r\n",
    "X_valid <- valid[, x]\r\n",
    "\r\n",
    "# Standardize X using X_train\r\n",
    "sc <- list(\r\n",
    "  center = attr(scale(data.matrix(X_train[, x_dense])), \"scaled:center\"),\r\n",
    "  scale = attr(scale(data.matrix(X_train[, x_dense])), \"scaled:scale\")\r\n",
    ")\r\n",
    "\r\n",
    "# Function that maps data.frame to scaled network input (a list with a dense part \r\n",
    "# and each embedding as separat integer vector)\r\n",
    "prep_nn <- function(X, dense = x_dense, emb = x_emb, scaling = sc) {\r\n",
    "  X_dense <- data.matrix(X[, dense, drop = FALSE])\r\n",
    "  X_dense <- scale(X_dense, center = scaling$center, scale = scaling$scale)\r\n",
    "  emb <- lapply(X[emb], function(x) as.integer(x) - 1)\r\n",
    "  c(list(dense1 = X_dense), emb)\r\n",
    "}\r\n",
    "\r\n",
    "# Inputs\r\n",
    "input_dense <- layer_input(shape = length(x_dense), name = \"dense1\")\r\n",
    "input_veh_body <- layer_input(shape = 1, name = \"veh_body\")\r\n",
    "\r\n",
    "# Embedding of veh_body\r\n",
    "emb_veh_body <- input_veh_body %>% \r\n",
    "  layer_embedding(input_dim = nlevels(dataCar$veh_body) + 1, \r\n",
    "                  output_dim = 1) %>% \r\n",
    "  layer_flatten()\r\n",
    "\r\n",
    "# Combine dense input and embedding\r\n",
    "outputs <- list(input_dense, emb_veh_body) %>% \r\n",
    "      layer_concatenate() %>% \r\n",
    "      layer_dense(30, activation = \"tanh\") %>% \r\n",
    "      layer_dense(1, activation = \"sigmoid\")\r\n",
    "\r\n",
    "# Input\r\n",
    "inputs <- list(dense1 = input_dense, \r\n",
    "               veh_body = input_veh_body)\r\n",
    "\r\n",
    "# Create and compile model\r\n",
    "nn <- keras_model(inputs = inputs, outputs = outputs)\r\n",
    "summary(nn)\r\n",
    "nn %>% compile(\r\n",
    "  optimizer = optimizer_adam(lr = 0.0001),\r\n",
    "  loss = 'binary_crossentropy'\r\n",
    ")\r\n",
    "\r\n",
    "# Callbacks\r\n",
    "cb <- list(\r\n",
    "  callback_early_stopping(patience = 20),\r\n",
    "  callback_reduce_lr_on_plateau(patience = 5)\r\n",
    ")\r\n",
    "       \r\n",
    "# Fit model\r\n",
    "history <- nn %>% fit(\r\n",
    "  x = prep_nn(X_train),\r\n",
    "  y = y_train,\r\n",
    "  epochs = 100,\r\n",
    "  batch_size = 400, \r\n",
    "  validation_data = list(prep_nn(X_valid), y_valid),\r\n",
    "  callbacks = cb\r\n",
    ")\r\n",
    "\r\n",
    "history$metrics[c(\"loss\", \"val_loss\")] %>% \r\n",
    "  data.frame() %>% \r\n",
    "  mutate(epoch = row_number()) %>% \r\n",
    "  pivot_longer(cols = c(\"loss\", \"val_loss\")) %>% \r\n",
    "ggplot(aes(x = epoch, y = value, group = name, color = name)) +\r\n",
    "  geom_line(size = 1.4)\r\n",
    "\r\n",
    "# Interpret\r\n",
    "library(flashlight)\r\n",
    "library(MetricsWeighted)\r\n",
    "\r\n",
    "fl <- flashlight(\r\n",
    "  model = nn, \r\n",
    "  y = y, \r\n",
    "  data = dataCar[ix$valid, ], \r\n",
    "  label = \"nn\", \r\n",
    "  metrics = list(logLoss = logLoss, `R squared` = r_squared_bernoulli),\r\n",
    "  predict_function = function(m, X) predict(m, prep_nn(X), batch_size = 1000)\r\n",
    ")\r\n",
    "\r\n",
    "# Performance on validation data\r\n",
    "plot(light_performance(fl), fill = \"orange\")\r\n",
    "\r\n",
    "# Permutation importance\r\n",
    "plot(light_importance(fl, v = x), fill = \"orange\")\r\n",
    "\r\n",
    "# Partial dependence\r\n",
    "plot(light_profile(fl, v = \"veh_value\", breaks = seq(0, 5, by = 0.1))) %>% \r\n",
    "  labs(title = \"Partial dependence plot for veh_value\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"veh_body\"), rotate_x = TRUE) +\r\n",
    "  labs(title = \"Partial dependence plot for veh_body\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"area\")) +\r\n",
    "  labs(title = \"Partial dependence plot for area\", y = \"price\")\r\n",
    "\r\n",
    "plot(light_profile(fl, v = \"agecat\")) +\r\n",
    "  labs(title = \"Partial dependence plot for agecat\", y = \"price\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises\r\n",
    "\r\n",
    "1. Fit diamond prices by gamma deviance loss with log-link (i.e. exponential output activation), using the custom loss function defined below. Tune the model by simple validation and evaluate it (for simplicity) on the validation data. Interpret the final model. (Hints: I used a smaller learning rate and had to replace the \"relu\" activations by \"tanh\".)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss_gamma <- function(y_true, y_pred) {\r\n",
    "  -k_log(y_true / y_pred) + y_true / y_pred\r\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Study either the optional claims data example or build your own neural net, predicting claim yes/no. For simplicity, you can represent the categorical feature `veh_body` by integers.\r\n",
    "\r\n",
    "# Neural Network Slang\r\n",
    "\r\n",
    "Here, we summarize some of the neural network slang.\r\n",
    "\r\n",
    "- Activation function: The transformation applied to the node values.\r\n",
    "- Architecture: The layout of layers and nodes.\r\n",
    "- Backpropagation: An efficient way to calculate gradients.\r\n",
    "- Batch: A couple of data rows used for one mini-batch gradient descent step.\r\n",
    "- Callback: An action during training, e.g. saving weights, reducing learning rate or stop training.\r\n",
    "- Epoch: The process of updating the network weights by gradient descent until each observation in the training set was used once.\r\n",
    "- Embedding: A numeric representation of categorical input as learned by the neural net.\r\n",
    "- Encoding: The values of latent variables of a hidden layer, usually the last.\r\n",
    "- Gradient descent: The basic optimization algorithm of neural networks.\r\n",
    "- Keras: User-friendly wrapper of TensorFlow.\r\n",
    "- Layer: Main organizational unit of a neural network.\r\n",
    "- Learning rate: Controls the step size of gradient descent, i.e. how aggressive the network learns.\r\n",
    "- Node: Nodes on the input layer are the covariates, nodes on the output layer the response(s) and nodes on a hidden layer are latent variables representing the covariates for the task to predict the response.\r\n",
    "- Optimizer: The specific variant of gradient descent.\r\n",
    "- PyTorch: An important implementation of neural networks.\r\n",
    "- Stochastic gradient descent (SGD): Mini-batch gradient descent with batches of size 1.\r\n",
    "- TensorFlow: An important implementation of neural networks.\r\n",
    "- Weights: The parameters of a neural net.\r\n",
    "\r\n",
    "# Chapter Summary\r\n",
    "\r\n",
    "In this chapter, we have glimpsed into the world of neural networks. Step by step we have learned how a neural network works. We have used Keras and TensorFlow to build models brick by brick.\r\n",
    "\r\n",
    "# Closing Remarks\r\n",
    "\r\n",
    "During this lecture, we have met many ML algorithms and principles. To get used to them, the best approach is practicing. [Kaggle](kaggle.com) is a great place to do so and learn from the best. \r\n",
    "\r\n",
    "A summary and comparison of the algorithms can be found on [github](https://github.com/mayer79/ML_Algorithm_Comparison). Here a screenshot as per Sept. 7, 2020: \r\n",
    "![](../figs/comparison_ML.PNG).\r\n",
    "\r\n",
    "# Chapter References\r\n",
    "\r\n",
    "[1] P.J. Werbos, \"Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences\", Dissertation, 1974.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ml_lecture': conda)"
  },
  "interpreter": {
   "hash": "a185c32b4de78f8a6ff83bd1757084efa54bdc6ffdb11531dca4c6078c748cc0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}